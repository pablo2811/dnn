{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxaMTckGDGgc"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6y4l5BmxTNNU"
      },
      "outputs": [],
      "source": [
        "from models.network import Network\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uutaqUkuVAuF",
        "outputId": "20b380a6-ca92-49d7-c090-1a6648c1605e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784) (60000, 10) float64 float64\n",
            "(10000, 784) (10000, 10) float64 float64\n"
          ]
        }
      ],
      "source": [
        "# Let's read the MNIST dataset (handwritten digit recognition)\n",
        "# 28 x 28 grayscale images: we flatten them here as vectors of length 784 and cast to float64 in 0..1 range.\n",
        "# More: https://en.wikipedia.org/wiki/MNIST_database\n",
        "\n",
        "def load_mnist(path: str = 'data/mnist.npz') -> tuple[tuple[np.ndarray, np.ndarray], tuple[np.ndarray, np.ndarray]]:\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f['x_train'], f['y_train']\n",
        "        x_test, _y_test = f['x_test'], f['y_test']\n",
        "\n",
        "    x_train = x_train.reshape(-1, 28 * 28) / 255.\n",
        "    x_test = x_test.reshape(-1, 28 * 28) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
        "print(x_train.shape, y_train.shape, x_train.dtype, y_train.dtype)\n",
        "print(x_test.shape, y_test.shape, x_test.dtype, y_test.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCU-YeNYqqV_",
        "outputId": "3997a596-10cc-4554-ddb2-746e227cb589"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
              "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train[0][300:350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GPG7v3SrNNG",
        "outputId": "0702410a-b94a-4a7a-c085-9306371e4918"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Labels are one-hot encoded.\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScY9bfCHXc16",
        "outputId": "b6e276c5-033b-4009-fc1f-c55858123173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.898\n",
            "Epoch: 1, Accuracy: 0.9144\n",
            "Epoch: 2, Accuracy: 0.9217\n",
            "Epoch: 3, Accuracy: 0.9231\n",
            "Epoch: 4, Accuracy: 0.9263\n",
            "Epoch: 5, Accuracy: 0.9309\n",
            "Epoch: 6, Accuracy: 0.9294\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/pfijalkowski/Development/Masters-ML/2023Z/DNN/dnn-impl/demo.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pfijalkowski/Development/Masters-ML/2023Z/DNN/dnn-impl/demo.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m network \u001b[39m=\u001b[39m Network([\u001b[39m784\u001b[39m, \u001b[39m30\u001b[39m, \u001b[39m10\u001b[39m], loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlog-loss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pfijalkowski/Development/Masters-ML/2023Z/DNN/dnn-impl/demo.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m network\u001b[39m.\u001b[39;49mSGD((x_train, y_train), epochs\u001b[39m=\u001b[39;49m\u001b[39m29\u001b[39;49m, mini_batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, eta\u001b[39m=\u001b[39;49m\u001b[39m3.\u001b[39;49m, test_data\u001b[39m=\u001b[39;49m(x_test, y_test))\n",
            "File \u001b[0;32m~/Development/Masters-ML/2023Z/DNN/dnn-impl/models/network.py:166\u001b[0m, in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mif\u001b[39;00m test_data:\n\u001b[1;32m    165\u001b[0m     x_test, y_test \u001b[39m=\u001b[39m test_data\n\u001b[0;32m--> 166\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m    167\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m mini_batch_size):\n\u001b[1;32m    168\u001b[0m         i_begin \u001b[39m=\u001b[39m i \u001b[39m*\u001b[39m mini_batch_size\n",
            "File \u001b[0;32m~/Development/Masters-ML/2023Z/DNN/dnn-impl/models/network.py:72\u001b[0m, in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, x_mini_batch, y_mini_batch, eta)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mMake a single step of backpropagation and gradient descent.\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m- eta: learning rate.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m nabla_b \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mzeros(b\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases]\n\u001b[0;32m---> 72\u001b[0m nabla_w \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mzeros(w\u001b[39m.\u001b[39mshape) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights]\n\u001b[1;32m     74\u001b[0m \u001b[39m# Accumulate gradients by running backprop one dataitem at a time (without vectorization).\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# for x, y in zip(x_mini_batch, y_mini_batch):\u001b[39;00m\n\u001b[1;32m     76\u001b[0m nabla_b, nabla_w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackprop(x_mini_batch, y_mini_batch)\n",
            "File \u001b[0;32m~/Development/Masters-ML/2023Z/DNN/dnn-impl/models/network.py:94\u001b[0m, in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(\u001b[39mself\u001b[39m, x: np\u001b[39m.\u001b[39mndarray, y: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mlist\u001b[39m[np\u001b[39m.\u001b[39mndarray], \u001b[39mlist\u001b[39m[np\u001b[39m.\u001b[39mndarray]]:\n\u001b[1;32m     86\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Backpropagation for a batch input.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m    - x: input, shape (B, I)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[39m    - y: target label (one-hot encoded), shape (B, O)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \n\u001b[1;32m     92\u001b[0m \u001b[39m    Returns (nabla_b, nabla_w), where:\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m    - nabla_b is a list of gradients over biases (shape (L_i)), for each layer.\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[39m    - nabla_w is a list of gradients over weights (shape (L_i, L_{i-1})), for each layer.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39m# Go forward, remembering all activations.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39m# Values before activation function, layer by layer, shapes (L_1), ..., (10).\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     fs, gs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedforward(x, \u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/Development/Masters-ML/2023Z/DNN/dnn-impl/models/network.py:43\u001b[0m, in \u001b[0;36mNetwork.feedforward\u001b[0;34m(self, a, memoize)\u001b[0m\n\u001b[1;32m     41\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m b, w \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbiases, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights):\n\u001b[0;32m---> 43\u001b[0m     fs\u001b[39m.\u001b[39mappend(gs[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m@\u001b[39;49m w\u001b[39m.\u001b[39;49mT  \u001b[39m+\u001b[39m b)\n\u001b[1;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     45\u001b[0m       gs_next \u001b[39m=\u001b[39m softmax(fs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "network = Network([784, 30, 10], loss=\"log-loss\")\n",
        "network.SGD((x_train, y_train), epochs=29, mini_batch_size=100, eta=3., test_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3weZ-gF_RkT"
      },
      "source": [
        "Vectorized version takes <2s per epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I50xq_UhoB8Y"
      },
      "source": [
        "# Excercise 3 (optional)\n",
        "\n",
        "Standart backpropagation method requires memorization of all outputs of all layers, which can take much of precious GPU memory.\n",
        "Instead of doing that, one can memorize only a select few layers and then recompute the rest as they are needed.\n",
        "Your task is to complete the code below to implement backpropagation with checkpoints.\n",
        "To keep things simple, use 1-example mini-batches (or, if you are bored, vectorize the code below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ks-sxtd6VrY"
      },
      "outputs": [],
      "source": [
        "class NetworkWithCheckpoints(object):\n",
        "    def __init__(self, sizes, checkpoints):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes) - 1\n",
        "        self.sizes = sizes\n",
        "        self.checkpoints = checkpoints\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x)\n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def feedforward_with_checkpoints(self, x):\n",
        "        # Runs network on a single case, memorizing the activations of layers included in checkpoints.\n",
        "        # Notice that gs (outputs of non-linearities) are shifted by one\n",
        "        fs = []\n",
        "        gs = [x]\n",
        "        g = x\n",
        "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            f = np.dot(w, g) + b\n",
        "            g = sigmoid(f)\n",
        "            if i in self.checkpoints:\n",
        "                fs.append(f)\n",
        "                gs.append(g)\n",
        "            else:\n",
        "                fs.append(None)\n",
        "                gs.append(None)\n",
        "        return fs, gs, g\n",
        "\n",
        "\n",
        "    def feedforward_between_layers(self, first_layer: int, last_layer: int, acc_f, acc_g):\n",
        "        # feedforward input acc_g[first_layer] for layers [first_layer, last_layer)\n",
        "        # memorizing their outputs in respective indexes of acc_f, acc_g\n",
        "\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def backprop_between_layers(self, start, end, acc_g, dLdg):\n",
        "        # compute the gradients for layers [start, end)\n",
        "        # dLdg is a gradient with respect to nonlinearity of layer[end-1]\n",
        "        # return changed dLdG so that it is gradient with respect to nonlinearieties of layer start-1\n",
        "        dLdWs = []\n",
        "        dLdBs = []\n",
        "\n",
        "        # TODO\n",
        "\n",
        "        return reversed(dLdWs), reversed(dLdBs), dLdg\n",
        "\n",
        "    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation with checkpoints to compute the gradient.\n",
        "        # For this exercise, we assume 1 element mini_batch\n",
        "        # eta is the learning rate\n",
        "        x_mini_batch = x_mini_batch.reshape(-1, 1)\n",
        "        y_mini_batch = y_mini_batch.reshape(-1, 1)\n",
        "\n",
        "        fs, gs, output = self.feedforward_with_checkpoints(x_mini_batch)\n",
        "        dLdg = output - y_mini_batch\n",
        "        for start, end in reversed(list(zip([-1] + self.checkpoints, self.checkpoints + [self.num_layers-1]))):\n",
        "            # if checkpoints are (a, b) then we can backprop through layers [a+1, b] inclusive\n",
        "            start += 1\n",
        "            end += 1\n",
        "            # those copies are inefficient, but we do them to keep indexing simple\n",
        "            acc_f = fs.copy()\n",
        "            acc_g = gs.copy()\n",
        "            self.feedforward_between_layers(start, end, acc_f, acc_g)\n",
        "            nabla_w, nabla_b, dLdg = self.backprop_between_layers(start, end, acc_g, dLdg)\n",
        "            self.weights[start:end] = [w - eta * dw for w, dw in zip(self.weights[start:end], nabla_w)]\n",
        "            self.biases[start:end] = [b - eta * db for b, db in zip(self.biases[start:end], nabla_b)]\n",
        "\n",
        "\n",
        "    def evaluate(self, x_test_data, y_test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [(np.argmax(self.feedforward(x_test_data[i].reshape(784,1))), np.argmax(y_test_data[i]))\n",
        "                        for i in range(len(x_test_data))]\n",
        "        # return accuracy\n",
        "        return np.mean([int(x == y) for (x, y) in test_results])\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y)\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n",
        "                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)]\n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test)))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq9yyg9toB8c"
      },
      "outputs": [],
      "source": [
        "network = NetworkWithCheckpoints([784,30, 30,10], checkpoints=[1])\n",
        "network.SGD((x_train, y_train), epochs=10, mini_batch_size=1, eta=0.01, test_data=(x_test, y_test) ) # per-example descend is really slow, try vectorizing it!\n",
        "# Just so you know, un-vectorized version takes about 25-35s per epoch"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
